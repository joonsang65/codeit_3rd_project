import os
import time
import asyncio
import hashlib
import json
from openai import AsyncOpenAI

class OpenAIClient:
    def __init__(self):  # .env íŒŒì¼ë¡œ api key ê´€ë¦¬í•´ì„œ ìœ í¬ë˜ì§€ ì•Šê²Œ í•˜ê¸°
        # main.pyì—ì„œ í•œë²ˆë§Œ ë¡œë“œë˜ë„ë¡ ìˆ˜ì •ë¨
        # base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))
        # env_path = os.path.join(base_dir, ".env")
        # load_dotenv(env_path)

        api_key = os.getenv("OPENAI_API_KEY")
        if api_key is None:
            raise ValueError(" !!! need to check .env or path !!! ")  # keyê°€ ì—†ê±°ë‚˜ íŒŒì¼ì´ ëˆ„ë½ëœ ê²½ìš° ì—ëŸ¬ ë°˜í™˜
        
        self.client = AsyncOpenAI(api_key=api_key)  # ë¹„ë™ê¸° ì²˜ë¦¬ ê°€ëŠ¥í•œ openai ëª¨ë¸ ì‚¬ìš©
        self.response_cache = {}  # ìºì‹± ë”•ì…”ë„ˆë¦¬
    
    def make_cache_key(self, system_prompt, user_prompt, temperature, few_shot_examples):
        payload = {  # ë¹„ìŠ·í•œ ì…ë ¥ ë“¤ì–´ì™”ì„ ë•Œ ìºì‹±ëœ ê°’ì„ í†µí•´ì„œ ìì› ì ˆì•½ì„ ìœ„í•¨
            "system": system_prompt,
            "user": user_prompt,
            "temp": temperature,
            "fewshot": few_shot_examples
        }
        raw = json.dumps(payload, sort_keys=True).encode()
        return hashlib.sha256(raw).hexdigest()  # ê° ê²°ê³¼ì— ëŒ€í•´ hash í‚¤ ê°’ìœ¼ë¡œ ì €ì¥í•´ë‘ 
    
    async def fetch_response(self, system_prompt, user_prompt, temperature, model="gpt-4.1-mini", few_shot_examples=None):
        """ë‹¨ì¼ ì‘ë‹µ ìƒì„±"""
        cache_key = self.make_cache_key(system_prompt, user_prompt, temperature, few_shot_examples)
        
        if cache_key in self.response_cache:
            return temperature, self.response_cache[cache_key]["content"], self.response_cache[cache_key]["elapsed"]
        
        messages = [{"role": "system", "content": system_prompt}]
        if few_shot_examples:  # few-shot ì˜ˆì‹œê°€ ìˆìœ¼ë©´  -> ì¶”í›„ ê¸°ëŠ¥ í™•ì¥ ì‹œì— few-shot ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°ë„ ê³ ë ¤í•¨
            messages.extend(few_shot_examples)
        messages.append({"role": "user", "content": user_prompt})
        
        start = time.time()  # ì‘ë‹µì‹œê°„ ë¡œê¹…ìš©  -> ì¶”í›„ì—ëŠ” ì‚­ì œ ê°€ëŠ¥
        response = await self.client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature
        )  # ë¹„ë™ê¸° ì²˜ë¦¬
        elapsed = time.time() - start  # ì‘ë‹µ ì†Œìš”ì‹œê°„ ê³„ì‚°  -> ì´ê²ƒë„ ì‚­ì œ ê°€ëŠ¥
        
        content = response.choices[0].message.content.strip()
        self.response_cache[cache_key] = {"content": content, "elapsed": elapsed}
        
        return temperature, content, elapsed
    
    async def generate_multiple_responses(self, system_prompt, user_prompt, model="gpt-4.1-mini", few_shot_examples=None, temperatures=None):
        """ì—¬ëŸ¬ ì˜¨ë„ ì„¤ì •ìœ¼ë¡œ ì‘ë‹µ ìƒì„±"""
        if temperatures is None:
            temperatures = [0.2, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
        
        tasks = [
            self.fetch_response(system_prompt, user_prompt, temp, model, few_shot_examples)
            for temp in temperatures
        ]
        results = await asyncio.gather(*tasks)  # ë¹„ë™ê¸° ì²˜ë¦¬
        return results
    
    async def run_generation(self, model_type: str, user_prompt: str, system_prompt: str, few_shot_examples=None):
        """ì „ì²´ ìƒì„± ê³¼ì • ì‹¤í–‰"""
        zero_set = time.time()  # ì‚¬ìš©ì ì…ë ¥ ì™„ë£Œ ì‹œê°„
        model_name = "gpt-4.1-mini" if model_type == "mini" else "gpt-4.1-nano"
        
        results = await self.generate_multiple_responses(
            system_prompt, user_prompt, model=model_name, few_shot_examples=few_shot_examples
        )
        
        print("\nâ–¶ ì‘ë‹µ ê²°ê³¼:")
        return results
        # for temp, output, elapsed in results:
        #     print(f"\nğŸŒ¡ Temperature {temp} (â± {elapsed:.2f}ì´ˆ):\n{output}")
        #     print("-" * 50)
        #     return f"\nğŸŒ¡ Temperature {temp} (â± {elapsed:.2f}ì´ˆ):\n{output}"
        
        # last_time = time.time()  # ê²°ê³¼ ì¶œë ¥ ì™„ë£Œ ì‹œê°„
        # print(f"\nğŸ“Š ì „ì²´ ì¸í¼ëŸ°ìŠ¤ ì‹œê°„: {(last_time - zero_set):.2f}ì´ˆ")