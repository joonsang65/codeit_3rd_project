import os, time, asyncio, hashlib, json
from dotenv import load_dotenv
from openai import AsyncOpenAI
import logging
from typing import List, Optional, Tuple, Dict, Any
from config import DEFAULT_MODEL, DEFAULT_TEMPERATURES, DEFAULT_ENV_PATH, PROMPT_CONFIGS

class OpenAIClient:
    def __init__(self, env_path=DEFAULT_ENV_PATH):
        """
        Args:
            env_path (str): .env íŒŒì¼ ê²½ë¡œ, ê¸°ë³¸ê°’ì€ DEFAULT_ENV_PATH

        Returns:
            None
        """
        load_dotenv(env_path)
        api_key = os.getenv("OPENAI_API_KEY")
        if api_key is None:
            raise ValueError(" !!! need to check .env or path !!! ")
        self.client = AsyncOpenAI(api_key=api_key)
        self.response_cache = {}

    def make_cache_key(self, system_prompt, user_prompt, temperature, few_shot_examples):
        """
        Args:
            system_prompt (str): ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸
            user_prompt (str): ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸
            temperature (float): ì˜¨ë„ê°’
            few_shot_examples (list or None): few-shot ì˜ˆì‹œ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸

        Returns:
            str: ìºì‹œ í‚¤ë¡œ ì‚¬ìš©í•  SHA256 í•´ì‹œ ë¬¸ìì—´
        """
        payload = {
            "system": system_prompt,
            "user": user_prompt,
            "temp": temperature,
            "fewshot": few_shot_examples
        }
        raw = json.dumps(payload, sort_keys=True).encode()
        return hashlib.sha256(raw).hexdigest()


    async def fetch_response(
        self,
        system_prompt: str,
        user_prompt: str,
        temperature: float,
        model: str,
        few_shot_examples: Optional[List[Dict[str, str]]] = None
    ) -> Tuple[float, str, float]:
        """
        OpenAI APIë¥¼ í†µí•´ ì‘ë‹µì„ ê°€ì ¸ì˜¤ê³ , ìºì‹œë¥¼ í™œìš©í•´ ì¤‘ë³µ ìš”ì²­ì„ ë°©ì§€í•¨.

        Args:
            system_prompt (str): ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            user_prompt (str): ìœ ì € ì…ë ¥ í”„ë¡¬í”„íŠ¸
            temperature (float): ìƒì„± ë‹¤ì–‘ì„± ì¡°ì ˆ íŒŒë¼ë¯¸í„°
            model (str): ì‚¬ìš©í•  OpenAI ëª¨ë¸ ì´ë¦„
            few_shot_examples (list): few-shot ì˜ˆì œ ë©”ì‹œì§€ (optional)

        Returns:
            tuple: (temperature, ìƒì„±ëœ í…ìŠ¤íŠ¸, ì‘ë‹µ ì†Œìš” ì‹œê°„)
        """
        try:
            cache_key = self.make_cache_key(system_prompt, user_prompt, temperature, few_shot_examples)
            if cache_key in self.response_cache:
                return (
                    temperature,
                    self.response_cache[cache_key]["content"],
                    self.response_cache[cache_key]["elapsed"],
                )

            messages = [{"role": "system", "content": system_prompt}]
            if few_shot_examples:
                messages.extend(few_shot_examples)
            messages.append({"role": "user", "content": user_prompt})

            start = time.time()
            response = await self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature
            )
            elapsed = time.time() - start
            content = response.choices[0].message.content.strip()
            self.response_cache[cache_key] = {"content": content, "elapsed": elapsed}
            return temperature, content, elapsed

        except Exception as e:
            logging.error(f"[fetch_response] ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            raise

    async def generate_texts(
        self,
        platforms: List[str],
        product_name: str,
        product_use: str,
        brand_name: str,
        extra_info: Optional[str],
        mode: str
    ) -> Dict[str, str]:
        """
        ë‹¨ì¼ temperatureë¡œ ê´‘ê³  ë¬¸êµ¬ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜.

        Args:
            platforms (list): ìƒì„± ëŒ€ìƒ í”Œë«í¼ ë¦¬ìŠ¤íŠ¸
            product_name (str): ìƒí’ˆ ì´ë¦„
            product_use (str): ìƒí’ˆ ìš©ë„
            brand_name (str): ë¸Œëœë“œëª…
            extra_info (str): ì¶”ê°€ ì •ë³´ (optional)
            mode (str): ìƒì„± ëª¨ë“œ ("ê´‘ê³  ë¬¸êµ¬ë§Œ ìƒì„±", ...)

        Returns:
            dict: {í”Œë«í¼: ìƒì„±ëœ ê´‘ê³  ë¬¸êµ¬}
        """
        try:
            results = {}
            total_elapsed = []

            prompt_parts = [
                f"ìƒí’ˆ ì´ë¦„: {product_name}",
                f"ìƒí’ˆ ìš©ë„: {product_use}",
                f"ë¸Œëœë“œëª…: {brand_name}",
            ]
            if extra_info:
                prompt_parts.append(f"ì¶”ê°€ ì •ë³´: {extra_info}")
            user_prompt = "\n".join(prompt_parts)

            if mode == "ê´‘ê³  ë¬¸êµ¬ë§Œ ìƒì„±":
                for platform in platforms:
                    system_prompt, few_shot_examples = PROMPT_CONFIGS[platform]
                    _, content, elapsed = await self.fetch_response(
                        system_prompt, user_prompt, temperature=0.7, model=DEFAULT_MODEL["mini"], few_shot_examples=few_shot_examples
                    )
                    results[platform] = content
                    total_elapsed.append(elapsed)
            else:
                if any(p in ["ì¸ìŠ¤íƒ€ê·¸ë¨", "ë¸”ë¡œê·¸"] for p in platforms):
                    for platform in platforms:
                        if platform in ["ì¸ìŠ¤íƒ€ê·¸ë¨", "ë¸”ë¡œê·¸"]:
                            system_prompt, few_shot_examples = PROMPT_CONFIGS[platform]
                            _, content, elapsed = await self.fetch_response(
                                system_prompt, user_prompt, temperature=0.7, model=DEFAULT_MODEL["mini"], few_shot_examples=few_shot_examples
                            )
                            results[platform] = content
                            total_elapsed.append(elapsed)

                    # í¬ìŠ¤í„° ì¶”ê°€ ìƒì„±
                    system_prompt_í¬ìŠ¤í„°, few_shot_examples_í¬ìŠ¤í„° = PROMPT_CONFIGS["í¬ìŠ¤í„°"]
                    _, content_í¬ìŠ¤í„°, elapsed_í¬ìŠ¤í„° = await self.fetch_response(
                        system_prompt_í¬ìŠ¤í„°, user_prompt, temperature=0.7, model=DEFAULT_MODEL["mini"], few_shot_examples=few_shot_examples_í¬ìŠ¤í„°
                    )
                    results["í¬ìŠ¤í„°"] = content_í¬ìŠ¤í„°
                    total_elapsed.append(elapsed_í¬ìŠ¤í„°)
                else:
                    if "í¬ìŠ¤í„°" in platforms:
                        system_prompt, few_shot_examples = PROMPT_CONFIGS["í¬ìŠ¤í„°"]
                        _, content, elapsed = await self.fetch_response(
                            system_prompt, user_prompt, temperature=0.7, model=DEFAULT_MODEL["mini"], few_shot_examples=few_shot_examples
                        )
                        results["í¬ìŠ¤í„°"] = content
                        total_elapsed.append(elapsed)

            print(f"\nğŸ“Š ì „ì²´ ì¸í¼ëŸ°ìŠ¤ ì‹œê°„: {max(total_elapsed):.2f}ì´ˆ")
            return results

        except Exception as e:
            logging.error(f"[generate_texts] ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            raise

    async def generate_multiple_responses(
        self,
        platforms: List[str],
        product_name: str,
        product_use: str,
        brand_name: str,
        extra_info: Optional[str] = None,
        mode: str = "ê´‘ê³  ë¬¸êµ¬ë§Œ ìƒì„±",
        temperatures: Optional[List[float]] = None
    ) -> Dict[str, List[Tuple[float, str, float]]]:
        """
        ì—¬ëŸ¬ temperature ì¡°í•©ìœ¼ë¡œ ê´‘ê³  ë¬¸êµ¬ë¥¼ ë³‘ë ¬ë¡œ ìƒì„±í•˜ëŠ” í•¨ìˆ˜.

        Args:
            platforms (list): ëŒ€ìƒ í”Œë«í¼ ë¦¬ìŠ¤íŠ¸
            product_name (str): ìƒí’ˆ ì´ë¦„
            product_use (str): ìƒí’ˆ ìš©ë„
            brand_name (str): ë¸Œëœë“œëª…
            extra_info (str): ì¶”ê°€ ì •ë³´ (optional)
            mode (str): ìƒì„± ëª¨ë“œ
            temperatures (list): ì‚¬ìš©í•  temperature ê°’ ë¦¬ìŠ¤íŠ¸ (optional)

        Returns:
            dict: {í”Œë«í¼: [(temp, ìƒì„±ëœ ë¬¸êµ¬, ì‘ë‹µ ì‹œê°„)]}
        """
        try:
            if temperatures is None:
                temperatures = DEFAULT_TEMPERATURES

            results = {}

            prompt_parts = [
                f"ìƒí’ˆ ì´ë¦„: {product_name}",
                f"ìƒí’ˆ ìš©ë„: {product_use}",
                f"ë¸Œëœë“œëª…: {brand_name}",
            ]
            if extra_info:
                prompt_parts.append(f"ì¶”ê°€ ì •ë³´: {extra_info}")
            user_prompt = "\n".join(prompt_parts)

            async def fetch_for_platform(system_prompt, few_shot_examples):
                tasks = [
                    self.fetch_response(system_prompt, user_prompt, temp, DEFAULT_MODEL["mini"], few_shot_examples)
                    for temp in temperatures
                ]
                return await asyncio.gather(*tasks)

            async def fetch_platform(platform: str):
                if platform == "í¬ìŠ¤í„°":
                    system_prompt, few_shot_examples = PROMPT_CONFIGS["í¬ìŠ¤í„°"]
                else:
                    system_prompt, few_shot_examples = PROMPT_CONFIGS.get(platform, ("", []))
                return platform, await fetch_for_platform(system_prompt, few_shot_examples)

            if mode == "ê´‘ê³  ë¬¸êµ¬ë§Œ ìƒì„±":
                platforms_to_fetch = platforms
            else:
                if any(p in ["ì¸ìŠ¤íƒ€ê·¸ë¨", "ë¸”ë¡œê·¸"] for p in platforms):
                    platforms_to_fetch = [p for p in platforms if p in ["ì¸ìŠ¤íƒ€ê·¸ë¨", "ë¸”ë¡œê·¸"]] + ["í¬ìŠ¤í„°"]
                else:
                    platforms_to_fetch = ["í¬ìŠ¤í„°"] if "í¬ìŠ¤í„°" in platforms else []

            start_time = time.time()
            fetch_tasks = [fetch_platform(p) for p in platforms_to_fetch]
            results_list = await asyncio.gather(*fetch_tasks)

            for platform, res in results_list:
                results[platform] = res

            print(f"\nğŸ“Š ì „ì²´ ì¸í¼ëŸ°ìŠ¤ ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")
            return results

        except Exception as e:
            logging.error(f"[generate_multiple_responses] ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            raise